---
title: Volvo IT Belgium IT Incident Management
description: IT incident management event logs from Volvo IT Belgium.
---

import ImplementationDisclaimer from '../../../components/ImplementationDisclaimer.astro';

This challenge provides a real-life event log from Volvo IT Belgium. The data relates to service management processes, specifically focusing on how IT incident tickets are resolved.

The dataset has been used in several process mining papers as a benchmark dataset for process mining techniques.



## Summary Statistics

| Metric | Dataset | Value |
| --- | --- | --- |
| Number of events | Incident Records | 65,533 |
| Number of events | Closed Problems | 6,660 |
| Number of events | Open Problems | 2,351 |
| Number of cases | Incident Records | 7,554 |
| Number of cases | Closed Problems | 1,487 |
| Number of cases | Open Problems | 819 |
| Filesize | Incident Records | TBD |
| Filesize | Closed Problems | TBD |
| Filesize | Open Problems | TBD |

## Resource Links
The dataset is hosted at the [4TU research data portal](https://data.4tu.nl/)

* **Data Files:**
    * [Incident Records](https://data.4tu.nl/datasets/0fc5c579-e544-4fab-9143-fab1f5192432)
    * [Closed Problems](https://data.4tu.nl/datasets/1987a2a6-9f5b-4b14-8d26-ab7056b17929)
    * [Open Problems](https://data.4tu.nl/datasets/7aafbf5b-97ae-48ba-bd0a-4d973a68cd35)


## Challenge Questions
The process owner is specifically interested in addressing the following questions:

1.  **Push-to-Front Mechanism:** Is there evidence that cases are pushed to the 2nd and 3rd line too often or too soon?

2. **Ping-Pong (Multi-hop) Behaviour:** How often do cases `ping pong' between teams and which teams are more or less involved in ping-ponging?

3. **Wait User Abuse:** Is the "wait user" substatus abused to hide problems with the total resolution time?

4. **Process Conformity per Organisation:** Where do the two IT organisations differ and why?

## Implementation Considerations

<ImplementationDisclaimer />

1. **Ping‑pong and agency definition:** Ping‑pong depends on how handoffs are defined and whether system actors (e.g., queues) are included. 
Excluding system users can clarify human collaboration but may hide automation that drives handoffs.

2. **Push‑to‑front classification:** Identifying 1st/2nd/3rd line teams from resource labels can misclassify queue statuses, yielding false positives when the queue is tagged as a higher line even if 1st line work continues.

3. **Status vs. substatus semantics:** Events are captured as status + substatus rather than a single activity label.
Analysts may concatenate them, choose one, or map to higher‑level events, each changing variant counts and conformance.

4. **Process conformity scope:** Conformity can be structural (event order) or temporal (durations/SLAs). Mixing or choosing one definition affects which organization looks “non‑conformant.”

5. **Completed vs. running cases and rare paths:** Excluding running cases and filtering rare variants improves readability but can remove stalled/abandoned work and exception handling, biasing performance and ping‑pong metrics toward the “happy path.”

6. **Volume‑driven drift:** Splitting logs when volume surges treats frequency changes as process drift. This can reveal operational shifts but can also conflate workload intensity with structural change.

7. **State‑as‑goal mapping:** Treating a wait or pending state as “success” assumes a direct mapping between system status and business outcome, which may be context‑dependent.

8. **Zero‑duration events:** Marking zero‑duration steps as anomalies can remove valid automated transitions, changing throughput and waiting‑time calculations.
